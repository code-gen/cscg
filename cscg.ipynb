{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './language_model/')\n",
    "\n",
    "# for suppressing T.save warnings\n",
    "# see https://discuss.pyT.org/t/got-warning-couldnt-retrieve-source-code-for-container/7689\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True | CUDA: 10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from functools import reduce\n",
    "import operator\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import StandardDataset\n",
    "from language_model.lm_train import train_language_model\n",
    "from language_model.lm_prob import LMProb\n",
    "from ml_utils.config import Config\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width=180, indent=2, compact=False)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()\n",
    "\n",
    "print(f'GPU: {T.cuda.is_available()} | CUDA: {T.version.cuda}')\n",
    "\n",
    "def from_home(x):\n",
    "    return os.path.join(os.environ['HOME'], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: conala-corpus\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR   = from_home('workspace/ml-data/msc-research')\n",
    "\n",
    "# DJANGO_DIR = os.path.join(ROOT_DIR, 'raw-datasets/testing') # simple django\n",
    "DJANGO_DIR = os.path.join(ROOT_DIR, 'raw-datasets/django')\n",
    "CONALA_DIR = os.path.join(ROOT_DIR, 'raw-datasets/conala-corpus')\n",
    "\n",
    "DATASET_DIR = CONALA_DIR\n",
    "EMB_DIR     = os.path.join(ROOT_DIR, 'embeddings')\n",
    "\n",
    "print(f'Dataset: {os.path.basename(DATASET_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(l.strip().split()) for l in open(DATASET_DIR + '/all.anno').readlines()]\n",
    "c = [len(l.strip().split()) for l in open(DATASET_DIR + '/all.code').readlines()]\n",
    "assert len(a) == len(c)\n",
    "\n",
    "d = pd.DataFrame([{'a': _a, 'c': _c} for (_a, _c) in zip(a, c)])\n",
    "d.describe()\n",
    "\n",
    "a = round(len(list(filter(lambda x: x <= 10, a))) / len(a), 3)\n",
    "c = round(len(list(filter(lambda x: x <= 10, c))) / len(c), 3)\n",
    "a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Construct config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Config() # main config\n",
    "\n",
    "# sub-config for dataset\n",
    "CFG.dataset_cfg = Config()\n",
    "CFG.dataset_cfg.__dict__ = {\n",
    "    'root_dir': DATASET_DIR + '/train',\n",
    "    'anno_min_freq': 1,\n",
    "    'code_min_freq': 1,\n",
    "    'anno_seq_maxlen': 10,\n",
    "    'code_seq_maxlen': 10,\n",
    "    'emb_file': os.path.join(EMB_DIR, 'glove.6B.50d.txt.pickle'),\n",
    "}\n",
    "\n",
    "dataset = StandardDataset(config=CFG.dataset_cfg)\n",
    "\n",
    "# sub-config for NL intents\n",
    "CFG.anno = Config() \n",
    "CFG.anno.__dict__ = {\n",
    "    'lstm_hidden_size': 64,\n",
    "    'lstm_dropout_p': 0.0,\n",
    "    'att_dropout_p': 0.0,\n",
    "    'lang': dataset.anno_lang,\n",
    "    'load_pretrained_emb': True,\n",
    "    'emb_size': 50,\n",
    "}\n",
    "\n",
    "# sub-config for source code\n",
    "CFG.code = Config() \n",
    "CFG.code.__dict__ = {\n",
    "    'lstm_hidden_size': 128,\n",
    "    'lstm_dropout_p': 0.0,\n",
    "    'att_dropout_p': 0.0,\n",
    "    'lang': dataset.code_lang,\n",
    "    'load_pretrained_emb': False,\n",
    "    'emb_size': 16,\n",
    "}\n",
    "\n",
    "CFG.__dict__.update({\n",
    "    'cuda': True,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks = dataset.code_lang.to_numeric('return dict', tokenize_mode='anno', pad_mode='post', max_len=10)\n",
    "# ws = dataset.code_lang.to_tokens(T.tensor(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = np.random.randint(len(dataset))\n",
    "# a, c = dataset[i]\n",
    "# assert len(a) == CFG.dataset_cfg.anno_seq_maxlen, f'{i}'\n",
    "# assert len(c) == CFG.dataset_cfg.code_seq_maxlen, f'{i}'\n",
    "# pp.pprint(a)\n",
    "# pp.pprint(dataset.anno_lang.to_tokens(a))\n",
    "# print('-'*120)\n",
    "# pp.pprint(c)\n",
    "# pp.pprint(dataset.code_lang.to_tokens(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2. Compute LM probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Get train/test/valid splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tp, _vp = 0.1, 0.2\n",
    "splits = dataset.train_test_valid_split(test_p=_tp, valid_p=_vp, seed=42)\n",
    "\n",
    "for kind in splits:\n",
    "    for t in splits[kind]:\n",
    "        vs = splits[kind][t]\n",
    "        vs = T.cat(vs)\n",
    "        vs = vs[vs != 0]\n",
    "        splits[kind][t] = vs\n",
    "        \n",
    "print(f'train {(1-_tp-_vp)*len(dataset):.2f} | test {_tp*len(dataset)} | dev {_vp*len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train language model\n",
    "\n",
    "**Note:** Must do this for both anno and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.language_model = Config()\n",
    "CFG.language_model.__dict__ = {\n",
    "    'dataset'     : 'conala',\n",
    "    'model'       : 'LSTM', # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)\n",
    "    'n_head'      : None,   # number of heads in the enc/dec of the Transformers\n",
    "    'emb_size'    : 32,     # size of the word embeddings\n",
    "    'n_hid'       : 64,     # number of hidden units per layer\n",
    "    'n_layers'    : 1,      # number of layers\n",
    "    'lr'          : 0.25,    # initial learning rate\n",
    "    'clip'        : 0.25,   # gradient clipping\n",
    "    'dropout_p'   : 0.05,    # dropout applied to layers\n",
    "    'tied'        : False,  # whether to tie the word embeddings and softmax weights\n",
    "    'log_interval': 100,\n",
    "    'epochs'      : 100, # upper epoch limit\n",
    "    'batch_size'  : 32,\n",
    "    'seed'        : None # for reproducibility\n",
    "}\n",
    "\n",
    "# CFG.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cfg = CFG.language_model\n",
    "\n",
    "for kind in ['anno', 'code']:\n",
    "    print(f'Training LM for {kind}\\n')\n",
    "\n",
    "    lm_cfg.kind = kind\n",
    "    lm_cfg.bptt = CFG.dataset_cfg.__dict__[f'{kind}_seq_maxlen'] # seq len\n",
    "    lm_cfg.save_path = f'./data/lm/lm-{lm_cfg.dataset}-{lm_cfg.kind}.pt' # path to save the final model\n",
    "    \n",
    "#     train_language_model(lm_cfg, \n",
    "#                          num_tokens=len(getattr(dataset, f'{kind}_lang')),\n",
    "#                          train_nums=T.stack(splits[kind]['train']),\n",
    "#                          test_nums=T.stack(splits[kind]['test']),\n",
    "#                          valid_nums=T.stack(splits[kind]['valid']))\n",
    "    \n",
    "    train_language_model(lm_cfg, \n",
    "                         num_tokens=len(getattr(dataset, f'{kind}_lang')),\n",
    "                         train_nums=splits[kind]['train'],\n",
    "                         test_nums=splits[kind]['test'],\n",
    "                         valid_nums=splits[kind]['valid'])\n",
    "    \n",
    "    print('*' * 120, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Compute LM probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61c915f473342c38a0e3bb0055d2864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='P(anno)', max=450.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7201492bd134bf3962c4d520e1327dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='P(code)', max=450.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm_paths = {k: f'./data/lm/lm-{CFG.language_model.dataset}-{k}.pt' for k in ['anno', 'code']}\n",
    "\n",
    "for f in lm_paths.values():\n",
    "    assert os.path.exists(f), f'Language Model: file <{f}> does not exist!'\n",
    "    \n",
    "_ = dataset.compute_lm_probs(lm_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(dataset))\n",
    "a, c, pa, pc = dataset[i]\n",
    "' '.join(dataset.anno_lang.to_tokens(a)[0]), ' '.join(dataset.code_lang.to_tokens(c)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyLMProb:\n",
    "#     def __init__(self, model_path):        \n",
    "#         self.model = T.load(open(model_path, 'rb'), map_location={'cuda:0': 'cpu'})\n",
    "#         self.model = self.model.cpu()\n",
    "#         self.model.eval()\n",
    "\n",
    "#     def get_prob(self, nums, verbose=False):\n",
    "#         with T.no_grad():\n",
    "#             inp = T.tensor([int(nums[0])]).long().unsqueeze(0)\n",
    "#             hidden = self.model.init_hidden(bsz=1)\n",
    "#             log_probs = []\n",
    "            \n",
    "#             for i in range(1, len(nums)):\n",
    "#                 output, hidden = self.model(inp, hidden)\n",
    "                \n",
    "#                 #word_weights = output.squeeze().data.double().exp()\n",
    "#                 #prob = word_weights[nums[i]] / word_weights.sum()\n",
    "#                 probs = F.softmax(output.squeeze(), dim=-1)\n",
    "#                 prob = probs[nums[i]]\n",
    "                \n",
    "#                 # append current log prob\n",
    "#                 log_probs += [T.log(prob)]\n",
    "#                 inp.data.fill_(int(nums[i]))\n",
    "\n",
    "#             if verbose:\n",
    "#                 for i in range(len(log_probs)):\n",
    "#                     print(f'{nums[i+1]:4d}: P(w|s) = {np.exp(log_probs[i]):8.4f} | logP(w|s) = {log_probs[i]:8.4f}')\n",
    "#                 print(f'=> sum_prob = {sum(log_probs):.4f}')\n",
    "\n",
    "#         return sum(log_probs) / len(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_probs = {'anno': [], 'code': []}\n",
    "\n",
    "# pad_idx = {\n",
    "#     'anno': dataset.anno_lang.token2index['<pad>'],\n",
    "#     'code': dataset.code_lang.token2index['<pad>']\n",
    "# } \n",
    "\n",
    "# for kind in lm_probs:\n",
    "#     lm = MyLMProb(lm_paths[kind])\n",
    "#     p = pad_idx[kind]\n",
    "\n",
    "#     for vec in tqdm(getattr(dataset, kind), total=len(dataset), desc=f'P({kind})'):\n",
    "#         lm_probs[kind] += [np.exp(lm.get_prob(vec[vec != pad_idx[kind]], verbose=False))]\n",
    "    \n",
    "#     lm_probs[kind] = sum(lm_probs[kind])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = 'anno'\n",
    "# lm = MyLMProb(lm_paths[kind])\n",
    "# s = {}\n",
    "# for t, i in tqdm(getattr(dataset, f'{kind}_lang').token2index.items()):\n",
    "#     if i in [0, 2, 3]:\n",
    "#         continue\n",
    "#     q = T.tensor([2, i, 3])\n",
    "#     s[i] = np.exp(lm.get_prob(q))\n",
    "    \n",
    "# xs, ys = zip(*sorted(s.items(), key=lambda k: -k[1]))\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# plt.bar(xs, ys)\n",
    "# plt.xticks(xs, rotation=90)\n",
    "\n",
    "# sum(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dual CS/CG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(config: Config):\n",
    "    emb = nn.Embedding(len(config.lang), config.emb_size, padding_idx=config.lang.pad_idx)\n",
    "    \n",
    "    if config.load_pretrained_emb:\n",
    "        assert config.lang.emb_matrix is not None\n",
    "        emb.weight = nn.Parameter(T.tensor(config.lang.emb_matrix, dtype=T.float32))\n",
    "        emb.weight.requires_grad = False\n",
    "        \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config: Config, model_type):\n",
    "        \"\"\"\n",
    "        :param model_type: cs / cg\n",
    "        cs: code -> anno\n",
    "        cg: anno -> code\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        assert model_type in ['cs', 'cg']\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        src_cfg = config.anno if model_type == 'cg' else config.code\n",
    "        tgt_cfg = config.code if model_type == 'cg' else config.anno\n",
    "        \n",
    "        # 1. ENCODER\n",
    "        self.src_embedding = get_embeddings(src_cfg)\n",
    "        self.encoder = nn.LSTM(input_size=src_cfg.emb_size,\n",
    "                               hidden_size=src_cfg.lstm_hidden_size,\n",
    "                               dropout=src_cfg.lstm_dropout_p,\n",
    "                               bidirectional=True,\n",
    "                               batch_first=True)\n",
    "        \n",
    "        self.decoder_cell_init_linear = nn.Linear(in_features=2*src_cfg.lstm_hidden_size,\n",
    "                                                  out_features=tgt_cfg.lstm_hidden_size)\n",
    "        \n",
    "        # 2. ATTENTION\n",
    "        # project source encoding to decoder rnn's h space (W from Luong score general)\n",
    "        self.att_src_W = nn.Linear(in_features=2*src_cfg.lstm_hidden_size,\n",
    "                                   out_features=tgt_cfg.lstm_hidden_size,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # transformation of decoder hidden states and context vectors before reading out target words\n",
    "        # this produces the attentional vector in (W from Luong eq. 5)\n",
    "        self.att_vec_W = nn.Linear(in_features=2*src_cfg.lstm_hidden_size + tgt_cfg.lstm_hidden_size,\n",
    "                                   out_features=tgt_cfg.lstm_hidden_size,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # 3. DECODER\n",
    "        self.tgt_embedding = get_embeddings(tgt_cfg)\n",
    "        self.decoder = nn.LSTMCell(input_size=tgt_cfg.emb_size + tgt_cfg.lstm_hidden_size,\n",
    "                                   hidden_size=tgt_cfg.lstm_hidden_size)\n",
    "       \n",
    "        # prob layer over target language\n",
    "        self.readout = nn.Linear(in_features=tgt_cfg.lstm_hidden_size,\n",
    "                                 out_features=len(tgt_cfg.lang),\n",
    "                                 bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(tgt_cfg.att_dropout_p)\n",
    "        \n",
    "        # 4. COPY MECHANISM\n",
    "        self.copy_gate = ... # TODO\n",
    "        \n",
    "        # save configs\n",
    "        self.src_cfg = src_cfg\n",
    "        self.tgt_cfg = tgt_cfg\n",
    "        \n",
    "        device = T.device('cuda' if CFG.cuda else 'cpu')\n",
    "        self.to(device)\n",
    "        print(f'[{model_type}] using [{device}]')\n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: bs, max_src_len\n",
    "        tgt: bs, max_tgt_len\n",
    "        \"\"\"\n",
    "        enc_out, (h0_dec, c0_dec) = self.encode(src)\n",
    "        scores, att_mats = self.decode(enc_out, h0_dec, c0_dec, tgt)\n",
    "        \n",
    "        return scores, att_mats\n",
    "    \n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        src : bs x max_src_len (emb look-up indices)\n",
    "        out : bs x max_src_len x 2*hid_size\n",
    "        h/c0: bs x tgt_hid_size\n",
    "        \"\"\"\n",
    "        emb = self.src_embedding(src)\n",
    "        out, (hn, cn) = self.encoder(emb) # hidden is zero by default\n",
    "        \n",
    "        # construct initial state for the decoder\n",
    "        c0_dec = self.decoder_cell_init_linear(T.cat([cn[0], cn[1]], dim=1))\n",
    "        h0_dec = c0_dec.tanh()\n",
    "        \n",
    "        return out, (h0_dec, c0_dec)\n",
    "    \n",
    "    \n",
    "    def decode(self, src_enc, h0_dec, c0_dec, tgt):\n",
    "        \"\"\"\n",
    "        src_enc: bs, max_src_len, 2*hid_size (== encoder output)\n",
    "        h/c0   : bs, tgt_hid_size\n",
    "        tgt    : bs, max_tgt_len (emb look-up indices)\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        scores, att_mats = [], []\n",
    "        \n",
    "        hidden = (h0_dec, c0_dec)\n",
    "        \n",
    "        emb = self.tgt_embedding(tgt) # bs, max_tgt_len, tgt_emb_size\n",
    "        \n",
    "        att_vec = T.zeros(batch_size, self.tgt_cfg.lstm_hidden_size, requires_grad=False)\n",
    "        if CFG.cuda:\n",
    "            att_vec = att_vec.cuda()\n",
    "        \n",
    "        # Luong W*hs: same for each timestep of the decoder\n",
    "        src_enc_att = self.att_src_W(src_enc) # bs, max_src_len, tgt_hid_size\n",
    "        \n",
    "        for t in range(tgt_len):\n",
    "            emb_t = emb[:, t, :]\n",
    "            x = T.cat([emb_t, att_vec], dim=-1)\n",
    "            h_t, c_t = self.decoder(x, hidden)\n",
    "\n",
    "            ctx_t, att_mat = self.luong_attention(h_t, src_enc, src_enc_att)\n",
    "            \n",
    "            # Luong eq. (5)\n",
    "            att_t = self.att_vec_W(T.cat([h_t, ctx_t], dim=1))\n",
    "            att_t = att_t.tanh()\n",
    "            att_t = self.dropout(att_t)\n",
    "            \n",
    "            # Luong eq. (6)\n",
    "            score_t = self.readout(att_t)\n",
    "            score_t = F.softmax(score_t, dim=-1)\n",
    "            \n",
    "            scores   += [score_t]\n",
    "            att_mats += [att_mat]\n",
    "            \n",
    "            # for next state t+1\n",
    "            att_vec = att_t\n",
    "            hidden  = (h_t, c_t)\n",
    "        \n",
    "        # bs, max_tgt_len, tgt_vocab_size\n",
    "        scores = T.stack(scores).permute((1, 0, 2))\n",
    "        \n",
    "        # each element: bs, max_src_len, max_tgt_len\n",
    "        att_mats = T.cat(att_mats, dim=1)\n",
    "        \n",
    "        return scores, att_mats\n",
    "            \n",
    "        \n",
    "    def luong_attention(self, h_t, src_enc, src_enc_att, mask=None):\n",
    "        \"\"\"\n",
    "        h_t               : bs, hid_size\n",
    "        src_enc (hs)      : bs, max_src_len, 2*src_hid_size \n",
    "        src_enc_att (W*hs): bs, max_src_len, tgt_hid_size\n",
    "        mask              : bs, max_src_len\n",
    "        \n",
    "        ctx_vec    : bs, 2*src_hid_size\n",
    "        att_weight : bs, max_src_len\n",
    "        att_mat    : bs, 1, max_src_len\n",
    "        \"\"\"\n",
    "        \n",
    "        # bs x src_max_len\n",
    "        score = T.bmm(src_enc_att, h_t.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        if mask:\n",
    "            score.data.masked_fill_(mask, -np.inf)\n",
    "        \n",
    "        att_mat = score.unsqueeze(1)\n",
    "        att_weights = F.softmax(score, dim=-1)\n",
    "        \n",
    "        # sum per timestep\n",
    "        ctx_vec = T.sum(att_weights.unsqueeze(2) * src_enc, dim=1)\n",
    "        \n",
    "        return ctx_vec, att_mat\n",
    "    \n",
    "    \n",
    "    def translate(self, src_sents, beam_size, to_words=True):\n",
    "        \"\"\"\n",
    "        Beam search\n",
    "        src: input sequence (anno / code)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_mask(lang):\n",
    "    mask = T.ones(len(lang))\n",
    "    mask[lang.token2index['<pad>']] = 0\n",
    "    return mask\n",
    "\n",
    "def JSD(a, b, clone=True, mask=None):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    assert a.shape == b.shape\n",
    "    _, n, _ = a.shape \n",
    "        \n",
    "    xa, xb = (a.clone(), b.clone()) if clone else (a, b)        \n",
    "    \n",
    "    xa = F.softmax(xa, dim=2) + eps\n",
    "    xb = F.softmax(xb, dim=2) + eps\n",
    "    \n",
    "    # common, averaged dist\n",
    "    avg = 0.5 * (xa + xb)\n",
    "    \n",
    "    # kl\n",
    "    xa = T.sum(xa * T.log(xa / avg), dim=2)\n",
    "    xb = T.sum(xb * T.log(xb / avg), dim=2)\n",
    "    \n",
    "    # js\n",
    "    xa = T.sum(xa, dim=1) / n\n",
    "    xb = T.sum(xb, dim=1) / n\n",
    "    \n",
    "    return 0.5 * (xa + xb)\n",
    "\n",
    "\n",
    "def JSD_2(A, B, mask=None):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    assert A.shape == B.shape\n",
    "    b, n, m = A.shape\n",
    "        \n",
    "    js = []\n",
    "    for bi in range(b):\n",
    "        kl_a, kl_b = 0, 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            a = A[bi, i, :]\n",
    "            b = B[bi, i, :]\n",
    "            \n",
    "            if mask is not None:\n",
    "                a[mask[i]] = -(1e8)\n",
    "                b[mask[i]] = -(1e8)\n",
    "            \n",
    "            a = F.softmax(a) + eps\n",
    "            b = F.softmax(b) + eps\n",
    "            m = 0.5 * (a + b)\n",
    "            kl_a += stats.entropy(a, m) / n\n",
    "            kl_b += stats.entropy(b, m) / n\n",
    "        \n",
    "        js += [0.5 * (kl_a + kl_b)]\n",
    "    \n",
    "    return T.tensor(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cg] using [cuda]\n",
      "[cs] using [cuda]\n",
      "DataLoader: 15 batches of size 32\n"
     ]
    }
   ],
   "source": [
    "cg_model     = Model(CFG, model_type='cg')\n",
    "cg_model.opt = O.Adam(lr=0.001, params=filter(lambda p: p.requires_grad, cg_model.parameters()))\n",
    "\n",
    "cs_model     = Model(CFG, model_type='cs')\n",
    "cs_model.opt = O.Adam(lr=0.001, params=filter(lambda p: p.requires_grad, cs_model.parameters()))\n",
    "\n",
    "kwargs = {} # {'num_workers': 4, 'pin_memory': True}\n",
    "train_loader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True, **kwargs)\n",
    "print(f'DataLoader: {len(train_loader)} batches of size {CFG.batch_size}')\n",
    "\n",
    "__cg_l = 0\n",
    "__cs_l = 0\n",
    "__att_l = 0\n",
    "__dual_l = 0\n",
    "__rep_every = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1/100 | Batch    15/   15 | avg CG 5.62171 | avg CS 6.37668 | avg ATT 0.07174 | avg DUAL 0.54698\n",
      "Epoch     2/100 | Batch    15/   15 | avg CG 4.08525 | avg CS 5.25362 | avg ATT 0.26653 | avg DUAL 0.25597\n",
      "Epoch     3/100 | Batch    15/   15 | avg CG 3.73128 | avg CS 4.65371 | avg ATT 0.05634 | avg DUAL 0.24676\n",
      "Epoch     4/100 | Batch    15/   15 | avg CG 3.57221 | avg CS 4.49362 | avg ATT 0.03460 | avg DUAL 0.23755\n",
      "Epoch     5/100 | Batch    15/   15 | avg CG 3.42223 | avg CS 4.41469 | avg ATT 0.03362 | avg DUAL 0.23454\n",
      "Epoch     6/100 | Batch    15/   15 | avg CG 3.29217 | avg CS 4.36132 | avg ATT 0.04067 | avg DUAL 0.25011\n",
      "Epoch     7/100 | Batch    15/   15 | avg CG 3.21287 | avg CS 4.33091 | avg ATT 0.08998 | avg DUAL 0.23442\n",
      "Epoch     8/100 | Batch    15/   15 | avg CG 2.96115 | avg CS 4.18690 | avg ATT 0.08064 | avg DUAL 0.24577\n",
      "Epoch     9/100 | Batch    15/   15 | avg CG 2.88731 | avg CS 4.15778 | avg ATT 0.08598 | avg DUAL 0.30170\n",
      "Epoch    10/100 | Batch    15/   15 | avg CG 2.69237 | avg CS 3.94458 | avg ATT 0.08841 | avg DUAL 0.32468\n",
      "Epoch    11/100 | Batch    15/   15 | avg CG 2.55829 | avg CS 3.92120 | avg ATT 0.08728 | avg DUAL 0.39484\n",
      "Epoch    12/100 | Batch    15/   15 | avg CG 2.32602 | avg CS 3.73759 | avg ATT 0.08067 | avg DUAL 0.40946\n",
      "Epoch    13/100 | Batch    15/   15 | avg CG 2.27260 | avg CS 3.74822 | avg ATT 0.10395 | avg DUAL 0.48528\n",
      "Epoch    14/100 | Batch    15/   15 | avg CG 2.10546 | avg CS 3.67471 | avg ATT 0.09627 | avg DUAL 0.55507\n",
      "Epoch    15/100 | Batch    15/   15 | avg CG 1.94770 | avg CS 3.53846 | avg ATT 0.09509 | avg DUAL 0.54881\n",
      "Epoch    16/100 | Batch    15/   15 | avg CG 1.86779 | avg CS 3.42560 | avg ATT 0.08309 | avg DUAL 0.57362\n",
      "Epoch    17/100 | Batch    15/   15 | avg CG 1.78393 | avg CS 3.39011 | avg ATT 0.08464 | avg DUAL 0.58233\n",
      "Epoch    18/100 | Batch    15/   15 | avg CG 1.62455 | avg CS 3.23975 | avg ATT 0.08072 | avg DUAL 0.59124\n",
      "Epoch    19/100 | Batch    15/   15 | avg CG 1.53634 | avg CS 3.23301 | avg ATT 0.08354 | avg DUAL 0.68306\n",
      "Epoch    20/100 | Batch    15/   15 | avg CG 1.43183 | avg CS 3.13602 | avg ATT 0.07474 | avg DUAL 0.68027\n",
      "Epoch    21/100 | Batch    15/   15 | avg CG 1.32461 | avg CS 2.97906 | avg ATT 0.08273 | avg DUAL 0.64884\n",
      "Epoch    22/100 | Batch    15/   15 | avg CG 1.22241 | avg CS 2.85035 | avg ATT 0.08211 | avg DUAL 0.64137\n",
      "Epoch    23/100 | Batch    15/   15 | avg CG 1.19426 | avg CS 2.84831 | avg ATT 0.08606 | avg DUAL 0.68390\n",
      "Epoch    24/100 | Batch    15/   15 | avg CG 1.09520 | avg CS 2.75728 | avg ATT 0.07645 | avg DUAL 0.69157\n",
      "Epoch    25/100 | Batch    15/   15 | avg CG 1.04967 | avg CS 2.78838 | avg ATT 0.11041 | avg DUAL 0.85769\n",
      "Epoch    26/100 | Batch    15/   15 | avg CG 0.97794 | avg CS 2.65442 | avg ATT 0.08752 | avg DUAL 0.73624\n",
      "Epoch    27/100 | Batch    15/   15 | avg CG 0.90883 | avg CS 2.57761 | avg ATT 0.07945 | avg DUAL 0.67806\n",
      "Epoch    28/100 | Batch    15/   15 | avg CG 0.83012 | avg CS 2.46547 | avg ATT 0.07439 | avg DUAL 0.67696\n",
      "Epoch    29/100 | Batch    15/   15 | avg CG 0.78734 | avg CS 2.39664 | avg ATT 0.07198 | avg DUAL 0.64436\n",
      "Epoch    30/100 | Batch    15/   15 | avg CG 0.75196 | avg CS 2.35116 | avg ATT 0.06515 | avg DUAL 0.62784\n",
      "Epoch    31/100 | Batch    15/   15 | avg CG 0.73015 | avg CS 2.35368 | avg ATT 0.06578 | avg DUAL 0.71027\n",
      "Epoch    32/100 | Batch    15/   15 | avg CG 0.69702 | avg CS 2.23154 | avg ATT 0.06540 | avg DUAL 0.60313\n",
      "Epoch    33/100 | Batch    15/   15 | avg CG 0.58865 | avg CS 2.14084 | avg ATT 0.05985 | avg DUAL 0.57341\n",
      "Epoch    34/100 | Batch    15/   15 | avg CG 0.55892 | avg CS 2.08888 | avg ATT 0.06694 | avg DUAL 0.57215\n",
      "Epoch    35/100 | Batch    15/   15 | avg CG 0.52854 | avg CS 2.12652 | avg ATT 0.06527 | avg DUAL 0.61853\n",
      "Epoch    36/100 | Batch    15/   15 | avg CG 0.48902 | avg CS 1.95333 | avg ATT 0.06348 | avg DUAL 0.50558\n",
      "Epoch    37/100 | Batch    15/   15 | avg CG 0.44794 | avg CS 1.87224 | avg ATT 0.05647 | avg DUAL 0.50277\n",
      "Epoch    38/100 | Batch    15/   15 | avg CG 0.43412 | avg CS 1.88260 | avg ATT 0.05620 | avg DUAL 0.52686\n",
      "Epoch    39/100 | Batch    15/   15 | avg CG 0.38856 | avg CS 1.78722 | avg ATT 0.05087 | avg DUAL 0.46394\n",
      "Epoch    40/100 | Batch    15/   15 | avg CG 0.40168 | avg CS 1.77058 | avg ATT 0.05044 | avg DUAL 0.46797\n",
      "Epoch    41/100 | Batch    15/   15 | avg CG 0.36690 | avg CS 1.77330 | avg ATT 0.08505 | avg DUAL 0.46771\n",
      "Epoch    42/100 | Batch    15/   15 | avg CG 0.32586 | avg CS 1.70440 | avg ATT 0.06334 | avg DUAL 0.45589\n",
      "Epoch    43/100 | Batch    15/   15 | avg CG 0.30227 | avg CS 1.61446 | avg ATT 0.05343 | avg DUAL 0.42249\n",
      "Epoch    44/100 | Batch    15/   15 | avg CG 0.28316 | avg CS 1.57117 | avg ATT 0.05262 | avg DUAL 0.39528\n",
      "Epoch    45/100 | Batch    15/   15 | avg CG 0.27670 | avg CS 1.58414 | avg ATT 0.04874 | avg DUAL 0.40526\n",
      "Epoch    46/100 | Batch    15/   15 | avg CG 0.25582 | avg CS 1.49751 | avg ATT 0.04903 | avg DUAL 0.42540\n",
      "Epoch    47/100 | Batch    15/   15 | avg CG 0.24302 | avg CS 1.42994 | avg ATT 0.04694 | avg DUAL 0.47572\n",
      "Epoch    48/100 | Batch    15/   15 | avg CG 0.21471 | avg CS 1.37951 | avg ATT 0.04556 | avg DUAL 0.38674\n",
      "Epoch    49/100 | Batch    15/   15 | avg CG 0.19554 | avg CS 1.31434 | avg ATT 0.04133 | avg DUAL 0.37139\n",
      "Epoch    50/100 | Batch    15/   15 | avg CG 0.18198 | avg CS 1.32915 | avg ATT 0.04010 | avg DUAL 0.37030\n",
      "Epoch    51/100 | Batch    15/   15 | avg CG 0.17183 | avg CS 1.29286 | avg ATT 0.04118 | avg DUAL 0.36753\n",
      "Epoch    52/100 | Batch    15/   15 | avg CG 0.15840 | avg CS 1.22183 | avg ATT 0.04004 | avg DUAL 0.38909\n",
      "Epoch    53/100 | Batch    15/   15 | avg CG 0.16351 | avg CS 1.28517 | avg ATT 0.03803 | avg DUAL 0.41778\n",
      "Epoch    54/100 | Batch    15/   15 | avg CG 0.14844 | avg CS 1.20396 | avg ATT 0.03737 | avg DUAL 0.37370\n",
      "Epoch    55/100 | Batch    15/   15 | avg CG 0.13160 | avg CS 1.12566 | avg ATT 0.03727 | avg DUAL 0.36846\n",
      "Epoch    56/100 | Batch    15/   15 | avg CG 0.12963 | avg CS 1.15361 | avg ATT 0.03867 | avg DUAL 0.36689\n",
      "Epoch    57/100 | Batch    15/   15 | avg CG 0.12620 | avg CS 1.12926 | avg ATT 0.03587 | avg DUAL 0.46364\n",
      "Epoch    58/100 | Batch    15/   15 | avg CG 0.12656 | avg CS 1.08221 | avg ATT 0.03675 | avg DUAL 0.38693\n",
      "Epoch    59/100 | Batch    15/   15 | avg CG 0.10594 | avg CS 1.03269 | avg ATT 0.03503 | avg DUAL 0.39412\n",
      "Epoch    60/100 | Batch    15/   15 | avg CG 0.09885 | avg CS 1.00735 | avg ATT 0.03392 | avg DUAL 0.38721\n",
      "Epoch    61/100 | Batch    15/   15 | avg CG 0.09499 | avg CS 1.00531 | avg ATT 0.03258 | avg DUAL 0.39170\n",
      "Epoch    62/100 | Batch    15/   15 | avg CG 0.08824 | avg CS 0.98643 | avg ATT 0.03353 | avg DUAL 0.39695\n",
      "Epoch    63/100 | Batch    15/   15 | avg CG 0.08472 | avg CS 0.93402 | avg ATT 0.03252 | avg DUAL 0.40778\n",
      "Epoch    64/100 | Batch    15/   15 | avg CG 0.08005 | avg CS 0.90991 | avg ATT 0.03380 | avg DUAL 0.41687\n",
      "Epoch    65/100 | Batch    15/   15 | avg CG 0.08120 | avg CS 0.95647 | avg ATT 0.04446 | avg DUAL 0.41030\n",
      "Epoch    66/100 | Batch    15/   15 | avg CG 0.08035 | avg CS 0.90219 | avg ATT 0.04414 | avg DUAL 0.42669\n",
      "Epoch    67/100 | Batch    15/   15 | avg CG 0.07282 | avg CS 0.84566 | avg ATT 0.03859 | avg DUAL 0.43525\n",
      "Epoch    68/100 | Batch    15/   15 | avg CG 0.07150 | avg CS 0.80439 | avg ATT 0.04283 | avg DUAL 0.46186\n",
      "Epoch    69/100 | Batch    15/   15 | avg CG 0.06603 | avg CS 0.80215 | avg ATT 0.03418 | avg DUAL 0.46456\n",
      "Epoch    70/100 | Batch    15/   15 | avg CG 0.06136 | avg CS 0.78946 | avg ATT 0.03152 | avg DUAL 0.49194\n",
      "Epoch    71/100 | Batch    15/   15 | avg CG 0.05825 | avg CS 0.75681 | avg ATT 0.03020 | avg DUAL 0.47864\n",
      "Epoch    72/100 | Batch    15/   15 | avg CG 0.05705 | avg CS 0.74795 | avg ATT 0.02941 | avg DUAL 0.48510\n",
      "Epoch    73/100 | Batch    15/   15 | avg CG 0.05410 | avg CS 0.73813 | avg ATT 0.02876 | avg DUAL 0.51182\n",
      "Epoch    74/100 | Batch    15/   15 | avg CG 0.05654 | avg CS 0.72830 | avg ATT 0.02745 | avg DUAL 0.57727\n",
      "Epoch    75/100 | Batch    15/   15 | avg CG 0.04974 | avg CS 0.68214 | avg ATT 0.02679 | avg DUAL 0.52432\n",
      "Epoch    76/100 | Batch    15/   15 | avg CG 0.04918 | avg CS 0.67705 | avg ATT 0.02761 | avg DUAL 0.54621\n",
      "Epoch    77/100 | Batch    15/   15 | avg CG 0.05043 | avg CS 0.72159 | avg ATT 0.02836 | avg DUAL 0.57534\n",
      "Epoch    78/100 | Batch    15/   15 | avg CG 0.04874 | avg CS 0.67502 | avg ATT 0.02702 | avg DUAL 0.56581\n",
      "Epoch    79/100 | Batch    15/   15 | avg CG 0.04343 | avg CS 0.62527 | avg ATT 0.02646 | avg DUAL 0.55293\n",
      "Epoch    80/100 | Batch    15/   15 | avg CG 0.04388 | avg CS 0.63583 | avg ATT 0.02639 | avg DUAL 0.61392\n",
      "Epoch    81/100 | Batch    15/   15 | avg CG 0.04328 | avg CS 0.60615 | avg ATT 0.03349 | avg DUAL 0.62492\n",
      "Epoch    82/100 | Batch    15/   15 | avg CG 0.04151 | avg CS 0.59064 | avg ATT 0.02793 | avg DUAL 0.65513\n",
      "Epoch    83/100 | Batch    15/   15 | avg CG 0.04111 | avg CS 0.59894 | avg ATT 0.02791 | avg DUAL 0.60942\n",
      "Epoch    84/100 | Batch    15/   15 | avg CG 0.04016 | avg CS 0.54277 | avg ATT 0.02784 | avg DUAL 0.61684\n",
      "Epoch    85/100 | Batch    15/   15 | avg CG 0.04028 | avg CS 0.55542 | avg ATT 0.02664 | avg DUAL 0.67975\n",
      "Epoch    86/100 | Batch    15/   15 | avg CG 0.03828 | avg CS 0.52235 | avg ATT 0.02318 | avg DUAL 0.67287\n",
      "Epoch    87/100 | Batch    15/   15 | avg CG 0.03849 | avg CS 0.53610 | avg ATT 0.02319 | avg DUAL 0.78024\n",
      "Epoch    88/100 | Batch    15/   15 | avg CG 0.03634 | avg CS 0.53512 | avg ATT 0.02763 | avg DUAL 0.70313\n",
      "Epoch    89/100 | Batch    15/   15 | avg CG 0.03407 | avg CS 0.50158 | avg ATT 0.02531 | avg DUAL 0.67965\n",
      "Epoch    90/100 | Batch    15/   15 | avg CG 0.03503 | avg CS 0.50541 | avg ATT 0.02840 | avg DUAL 0.75764\n",
      "Epoch    91/100 | Batch    15/   15 | avg CG 0.03265 | avg CS 0.46532 | avg ATT 0.02400 | avg DUAL 0.70303\n",
      "Epoch    92/100 | Batch    15/   15 | avg CG 0.03345 | avg CS 0.45848 | avg ATT 0.02236 | avg DUAL 0.78574\n",
      "Epoch    93/100 | Batch    15/   15 | avg CG 0.03274 | avg CS 0.43950 | avg ATT 0.02383 | avg DUAL 0.74730\n",
      "Epoch    94/100 | Batch    15/   15 | avg CG 0.03116 | avg CS 0.44527 | avg ATT 0.02222 | avg DUAL 0.76894\n",
      "Epoch    95/100 | Batch    15/   15 | avg CG 0.03144 | avg CS 0.42432 | avg ATT 0.02182 | avg DUAL 0.81471\n",
      "Epoch    96/100 | Batch    15/   15 | avg CG 0.03058 | avg CS 0.42434 | avg ATT 0.02222 | avg DUAL 0.77101\n",
      "Epoch    97/100 | Batch    15/   15 | avg CG 0.03062 | avg CS 0.40710 | avg ATT 0.02038 | avg DUAL 0.87716\n",
      "Epoch    98/100 | Batch    15/   15 | avg CG 0.02974 | avg CS 0.39420 | avg ATT 0.01914 | avg DUAL 0.84629\n",
      "Epoch    99/100 | Batch    15/   15 | avg CG 0.02885 | avg CS 0.36989 | avg ATT 0.01875 | avg DUAL 0.84521\n",
      "Epoch   100/100 | Batch    15/   15 | avg CG 0.02915 | avg CS 0.36799 | avg ATT 0.01886 | avg DUAL 0.91810\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(1, CFG.num_epochs+1):\n",
    "    \n",
    "    for batch_idx, (anno, code, anno_lm_p, code_lm_p) in enumerate(train_loader, start=1):        \n",
    "        anno_len, code_len = anno.shape[1], code.shape[1]\n",
    "        \n",
    "        if CFG.cuda:\n",
    "            anno, code, anno_lm_p, code_lm_p = map(lambda t: t.cuda(), [anno, code, anno_lm_p, code_lm_p])\n",
    "            \n",
    "        # binary mask indicating the presence of padding token\n",
    "        anno_mask = T.tensor(anno == dataset.anno_lang.token2index['<pad>']).byte()\n",
    "        code_mask = T.tensor(code == dataset.code_lang.token2index['<pad>']).byte()\n",
    "            \n",
    "        # forward pass\n",
    "        code_pred, code_att_mat = cg_model(src=anno, tgt=code)\n",
    "        anno_pred, anno_att_mat = cs_model(src=code, tgt=anno)\n",
    "                                \n",
    "        # loss computation\n",
    "        l_cg_ce, l_cs_ce = 0, 0\n",
    "        \n",
    "        # CG cross-entropy loss\n",
    "        for t in range(code_len):\n",
    "            probs = code_pred[:, t, :].gather(1, code[:, t].view(-1, 1)).squeeze(1)\n",
    "            l_cg_ce += -T.log(probs) / code_len\n",
    "                    \n",
    "        # CS cross-entropy loss\n",
    "        for t in range(anno_len):\n",
    "            probs = anno_pred[:, t, :].gather(1, anno[:, t].view(-1, 1)).squeeze(1)\n",
    "            l_cs_ce += -T.log(probs) / anno_len\n",
    "            \n",
    "        # dual loss: P(x,y) = P(x).P(y|x) = P(y).P(x|y)\n",
    "        l_dual = (code_lm_p - l_cs_ce - anno_lm_p + l_cg_ce) ** 2\n",
    "                \n",
    "        # attention loss: JSD\n",
    "        l_att = JSD(anno_att_mat, code_att_mat.transpose(2,1), mask=code_mask) + \\\n",
    "                JSD(anno_att_mat.transpose(2,1), code_att_mat, mask=anno_mask)\n",
    "                \n",
    "        # final loss\n",
    "        l_cg = T.mean(l_cg_ce + 0.01 * l_dual + 0.2 * l_att)\n",
    "        l_cs = T.mean(l_cs_ce + 0.01 * l_dual + 0.2 * l_att)\n",
    "                \n",
    "        # optimize CG\n",
    "        cg_model.opt.zero_grad()\n",
    "        l_cg.backward(retain_graph=True)\n",
    "        cg_model.opt.step()\n",
    "                \n",
    "        # optimize CS\n",
    "        cs_model.opt.zero_grad()\n",
    "        l_cs.backward()\n",
    "        cs_model.opt.step()\n",
    "                \n",
    "        # reporting\n",
    "        __cg_l   += l_cg.item()   / __rep_every\n",
    "        __cs_l   += l_cs.item()   / __rep_every\n",
    "        __att_l  += l_att.mean().item()  / __rep_every\n",
    "        __dual_l += l_dual.mean().item() / __rep_every\n",
    "        \n",
    "        if batch_idx % __rep_every == 0:\n",
    "            status = [f'Epoch {epoch_idx:>5d}/{CFG.num_epochs:>3d}', f'Batch {batch_idx:>5d}/{len(train_loader):5d}',\n",
    "                      f'avg CG {__cg_l:7.5f}', f'avg CS {__cs_l:7.5f}', f'avg ATT {__att_l:7.5f}', f'avg DUAL {__dual_l:7.5f}']\n",
    "            print(' | '.join(status))\n",
    "            __cg_l, __cs_l, __att_l, __dual_l = 0, 0, 0, 0\n",
    "    # --- epoch end\n",
    "            \n",
    "    # TODO...\n",
    "#     if epoch_idx % 1 == 0:\n",
    "#         with T.no_grad():\n",
    "#             print()\n",
    "#             ws = dataset.code_lang.to_tokens(code_pred.argmax(dim=-1))\n",
    "#             i = np.random.randint(len(ws))\n",
    "#             print(f'{i} pred: {\" \".join(ws[i])}')\n",
    "#             print(f'{i}  tgt: {\" \".join(code[i].argmax())}')\n",
    "            \n",
    "#             print('\\t'+'-'*80)\n",
    "#             ws = dataset.anno_lang.to_tokens(anno_pred.argmax(dim=-1))\n",
    "#             i = np.random.randint(len(ws))\n",
    "#             print(f'\\t{i}: {\" \".join(ws[i])}')\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dbf4397ce64f68b5696911692319fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         em_anno: 0.62208\n",
      "         em_code: 0.84286\n",
      "  strict_em_anno: 0.01299\n",
      "  strict_em_code: 0.19481\n"
     ]
    }
   ],
   "source": [
    "test_cfg = Config()\n",
    "test_cfg.__dict__ = {\n",
    "    'root_dir': DATASET_DIR + '/test',\n",
    "    'anno_min_freq': 1,\n",
    "    'code_min_freq': 1,\n",
    "    'anno_seq_maxlen': 10,\n",
    "    'code_seq_maxlen': 10,\n",
    "    'emb_file': os.path.join(EMB_DIR, 'glove.6B.50d.txt.pickle'),\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(StandardDataset(config=test_cfg), batch_size=1, shuffle=False)\n",
    "\n",
    "metrics = {k: 0 for k in [\n",
    "    'em_anno', 'em_code', 'strict_em_anno', 'strict_em_code'\n",
    "]}\n",
    "\n",
    "with T.no_grad():\n",
    "    for batch_idx, (anno, code) in tqdm(enumerate(test_loader, start=1), total=len(test_loader)): \n",
    "        if CFG.cuda:\n",
    "            anno, code, anno_lm_p, code_lm_p = map(lambda t: t.cuda(), [anno, code, anno_lm_p, code_lm_p])\n",
    "            \n",
    "        # forward pass\n",
    "        code_pred, code_att_mat = cg_model(src=anno, tgt=code)\n",
    "        anno_pred, anno_att_mat = cs_model(src=code, tgt=anno)\n",
    "        \n",
    "        # TODO: ideally, this should be beam-search\n",
    "        code_pred = code_pred.argmax(dim=2)\n",
    "        anno_pred = anno_pred.argmax(dim=2)\n",
    "        \n",
    "        code_score = T.mean((code_pred == code).float()).cpu()\n",
    "        anno_score = T.mean((anno_pred == anno).float()).cpu()\n",
    "        \n",
    "        metrics['em_code'] += code_score / len(test_loader)\n",
    "        metrics['em_anno'] += anno_score / len(test_loader)\n",
    "        \n",
    "        if np.isclose(code_score, 1):\n",
    "            metrics['strict_em_code'] += 1 / len(test_loader)\n",
    "        if np.isclose(anno_score, 1):\n",
    "            metrics['strict_em_anno'] += 1 / len(test_loader)\n",
    "        \n",
    "for k, v in metrics.items():\n",
    "    print(f'{k:>16s}: {v:7.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T.no_grad():\n",
    "    a, c = anno[[1]], code[[1]]\n",
    "    x, x_mat = cg_model(src=a, tgt=c)\n",
    "    y, y_mat = cs_model(src=c, tgt=a)\n",
    "    x = x[0].cpu()\n",
    "    x_mat = x_mat[0].cpu()\n",
    "    y = y[0].cpu()\n",
    "    y_mat = y_mat[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = dataset.code_lang.to_tokens(c)[0]\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = dataset.anno_lang.to_tokens(a)[0]\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = dataset.code_lang.to_tokens(x.argmax(dim=1))[0]\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = dataset.anno_lang.to_tokens(y.argmax(dim=1))[0]\n",
    "yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(F.softmax(x_mat, -1), cmap='jet')\n",
    "plt.grid(False)\n",
    "plt.xticks(ticks=np.arange(len(yt)), labels=yt, rotation=90)\n",
    "plt.xlabel('anno')\n",
    "plt.yticks(ticks=np.arange(len(xt)), labels=xt, rotation=0)\n",
    "plt.ylabel('code')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(F.softmax(y_mat, -1), cmap='jet')\n",
    "plt.grid(False)\n",
    "plt.xticks(ticks=np.arange(len(ct)), labels=ct, rotation=90)\n",
    "plt.xlabel('code')\n",
    "plt.yticks(ticks=np.arange(len(yt)), labels=yt, rotation=0)\n",
    "plt.ylabel('anno')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
